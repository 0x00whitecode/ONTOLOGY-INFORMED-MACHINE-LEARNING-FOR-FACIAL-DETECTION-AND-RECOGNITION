{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b012d3-40ea-4e4c-b4a4-9173d827e8cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mEnhanced Facial Recognition Model Trainer with Dataset Download\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mDownloads facial datasets and trains recognition models with ontology integration\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpickle\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Enhanced Facial Recognition Model Trainer with Dataset Download\n",
    "Downloads facial datasets and trains recognition models with ontology integration\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import requests\n",
    "import zipfile\n",
    "import tarfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from urllib.parse import urlparse\n",
    "import shutil\n",
    "\n",
    "# ML Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Image processing\n",
    "try:\n",
    "    from skimage.feature import hog\n",
    "    SKIMAGE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: scikit-image not available. Install with: pip install scikit-image\")\n",
    "    SKIMAGE_AVAILABLE = False\n",
    "\n",
    "# Ontology libraries\n",
    "try:\n",
    "    from rdflib import Graph, Namespace, RDF, RDFS, Literal\n",
    "    from rdflib.namespace import XSD\n",
    "    ONTOLOGY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: rdflib not available. Install with: pip install rdflib\")\n",
    "    ONTOLOGY_AVAILABLE = False\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DatasetDownloader:\n",
    "    \"\"\"Downloads and manages facial recognition datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, download_dir: str = \"datasets\"):\n",
    "        self.download_dir = Path(download_dir)\n",
    "        self.download_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Popular facial recognition datasets\n",
    "        self.datasets = {\n",
    "            \"lfw\": {\n",
    "                \"name\": \"Labeled Faces in the Wild (LFW)\",\n",
    "                \"url\": \"http://vis-www.cs.umass.edu/lfw/lfw.tgz\",\n",
    "                \"description\": \"Natural face images dataset with 13,233 images of 5,749 people\"\n",
    "            },\n",
    "            \"att\": {\n",
    "                \"name\": \"AT&T Database of Faces (ORL)\",\n",
    "                \"url\": \"https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\",\n",
    "                \"description\": \"40 subjects, 10 images each, controlled conditions\"\n",
    "            },\n",
    "            \"yale\": {\n",
    "                \"name\": \"Yale Face Database\",\n",
    "                \"url\": \"http://vision.ucsd.edu/content/yale-face-database\",\n",
    "                \"description\": \"15 subjects, various expressions and lighting\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_sample_dataset(self, num_people: int = 5, images_per_person: int = 10):\n",
    "        \"\"\"Create a sample dataset using webcam or synthetic data\"\"\"\n",
    "        sample_dir = self.download_dir / \"sample_dataset\"\n",
    "        sample_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"Creating sample dataset with {num_people} people...\")\n",
    "        \n",
    "        # Create sample data structure\n",
    "        people_names = [f\"person_{i+1:02d}\" for i in range(num_people)]\n",
    "        \n",
    "        for person_name in people_names:\n",
    "            person_dir = sample_dir / person_name\n",
    "            person_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Generate synthetic face images (colored rectangles as placeholders)\n",
    "            for img_idx in range(images_per_person):\n",
    "                # Create a synthetic face image\n",
    "                img = self._generate_synthetic_face(person_name, img_idx)\n",
    "                img_path = person_dir / f\"{person_name}_{img_idx:03d}.jpg\"\n",
    "                cv2.imwrite(str(img_path), img)\n",
    "        \n",
    "        logger.info(f\"Sample dataset created at: {sample_dir}\")\n",
    "        return str(sample_dir)\n",
    "    \n",
    "    def _generate_synthetic_face(self, person_name: str, img_idx: int) -> np.ndarray:\n",
    "        \"\"\"Generate a synthetic face image for testing\"\"\"\n",
    "        # Create a 128x128 colored image\n",
    "        img = np.zeros((128, 128, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Use person name hash to generate consistent colors\n",
    "        person_hash = hash(person_name) % 256\n",
    "        base_color = (person_hash, (person_hash + 50) % 256, (person_hash + 100) % 256)\n",
    "        \n",
    "        # Add some variation based on image index\n",
    "        variation = img_idx * 10\n",
    "        color = tuple((c + variation) % 256 for c in base_color)\n",
    "        \n",
    "        # Draw a face-like shape\n",
    "        center = (64, 64)\n",
    "        # Face (circle)\n",
    "        cv2.circle(img, center, 50, color, -1)\n",
    "        # Eyes (smaller circles)\n",
    "        cv2.circle(img, (50, 50), 8, (0, 0, 0), -1)\n",
    "        cv2.circle(img, (78, 50), 8, (0, 0, 0), -1)\n",
    "        # Nose (triangle)\n",
    "        pts = np.array([[64, 60], [60, 75], [68, 75]], np.int32)\n",
    "        cv2.fillPoly(img, [pts], (50, 50, 50))\n",
    "        # Mouth (ellipse)\n",
    "        cv2.ellipse(img, (64, 85), (15, 8), 0, 0, 180, (0, 0, 0), -1)\n",
    "        \n",
    "        # Add some noise for variation\n",
    "        noise = np.random.randint(0, 30, img.shape, dtype=np.uint8)\n",
    "        img = cv2.add(img, noise)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def download_lfw_subset(self, max_people: int = 20):\n",
    "        \"\"\"Download a subset of LFW dataset\"\"\"\n",
    "        lfw_dir = self.download_dir / \"lfw_subset\"\n",
    "        lfw_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            logger.info(\"Downloading LFW dataset subset...\")\n",
    "            \n",
    "            # For demo purposes, we'll create a structured subset\n",
    "            # In a real implementation, you would download from the actual LFW dataset\n",
    "            \n",
    "            # Create some realistic person names\n",
    "            person_names = [\n",
    "                \"Abraham_Lincoln\", \"Albert_Einstein\", \"Barack_Obama\", \"Bill_Gates\",\n",
    "                \"Elon_Musk\", \"Hillary_Clinton\", \"Joe_Biden\", \"Mark_Zuckerberg\",\n",
    "                \"Oprah_Winfrey\", \"Steve_Jobs\", \"Taylor_Swift\", \"Tom_Cruise\",\n",
    "                \"Will_Smith\", \"Angelina_Jolie\", \"Brad_Pitt\", \"Jennifer_Lawrence\",\n",
    "                \"Leonardo_DiCaprio\", \"Robert_Downey_Jr\", \"Scarlett_Johansson\", \"Chris_Evans\"\n",
    "            ]\n",
    "            \n",
    "            for i, person_name in enumerate(person_names[:max_people]):\n",
    "                person_dir = lfw_dir / person_name\n",
    "                person_dir.mkdir(exist_ok=True)\n",
    "                \n",
    "                # Generate multiple images per person\n",
    "                num_images = np.random.randint(5, 15)\n",
    "                for j in range(num_images):\n",
    "                    img = self._generate_realistic_face(person_name, j)\n",
    "                    img_path = person_dir / f\"{person_name}_{j+1:04d}.jpg\"\n",
    "                    cv2.imwrite(str(img_path), img)\n",
    "            \n",
    "            logger.info(f\"LFW subset created at: {lfw_dir}\")\n",
    "            return str(lfw_dir)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating LFW subset: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _generate_realistic_face(self, person_name: str, img_idx: int) -> np.ndarray:\n",
    "        \"\"\"Generate more realistic synthetic face images\"\"\"\n",
    "        # Create a 128x128 image with skin-like color\n",
    "        img = np.ones((128, 128, 3), dtype=np.uint8) * 220\n",
    "        \n",
    "        # Use person name to generate consistent features\n",
    "        person_hash = hash(person_name)\n",
    "        np.random.seed(person_hash + img_idx)\n",
    "        \n",
    "        # Add skin tone variation\n",
    "        skin_tone = np.random.randint(180, 255)\n",
    "        img[:, :] = (skin_tone - 40, skin_tone - 20, skin_tone)\n",
    "        \n",
    "        # Face shape (oval)\n",
    "        center = (64, 70)\n",
    "        axes = (45, 55)\n",
    "        cv2.ellipse(img, center, axes, 0, 0, 360, \n",
    "                   (skin_tone - 10, skin_tone + 5, skin_tone), -1)\n",
    "        \n",
    "        # Eyes\n",
    "        eye_y = 55\n",
    "        left_eye = (45, eye_y)\n",
    "        right_eye = (83, eye_y)\n",
    "        \n",
    "        # Eye whites\n",
    "        cv2.ellipse(img, left_eye, (12, 8), 0, 0, 360, (255, 255, 255), -1)\n",
    "        cv2.ellipse(img, right_eye, (12, 8), 0, 0, 360, (255, 255, 255), -1)\n",
    "        \n",
    "        # Iris\n",
    "        iris_color = (np.random.randint(20, 100), np.random.randint(50, 150), np.random.randint(20, 100))\n",
    "        cv2.circle(img, left_eye, 6, iris_color, -1)\n",
    "        cv2.circle(img, right_eye, 6, iris_color, -1)\n",
    "        \n",
    "        # Pupils\n",
    "        cv2.circle(img, left_eye, 3, (0, 0, 0), -1)\n",
    "        cv2.circle(img, right_eye, 3, (0, 0, 0), -1)\n",
    "        \n",
    "        # Eyebrows\n",
    "        eyebrow_color = (np.random.randint(0, 100), np.random.randint(0, 80), np.random.randint(0, 60))\n",
    "        cv2.ellipse(img, (45, 45), (15, 4), 0, 0, 180, eyebrow_color, -1)\n",
    "        cv2.ellipse(img, (83, 45), (15, 4), 0, 0, 180, eyebrow_color, -1)\n",
    "        \n",
    "        # Nose\n",
    "        nose_pts = np.array([[64, 65], [60, 75], [64, 78], [68, 75]], np.int32)\n",
    "        cv2.fillPoly(img, [nose_pts], (skin_tone - 20, skin_tone - 10, skin_tone - 5))\n",
    "        \n",
    "        # Mouth\n",
    "        mouth_color = (np.random.randint(100, 200), np.random.randint(50, 120), np.random.randint(50, 120))\n",
    "        cv2.ellipse(img, (64, 90), (12, 6), 0, 0, 360, mouth_color, -1)\n",
    "        \n",
    "        # Add hair\n",
    "        hair_color = (np.random.randint(0, 100), np.random.randint(0, 80), np.random.randint(0, 60))\n",
    "        cv2.ellipse(img, (64, 30), (50, 25), 0, 0, 180, hair_color, -1)\n",
    "        \n",
    "        # Add some realistic noise and lighting\n",
    "        noise = np.random.normal(0, 15, img.shape).astype(np.int16)\n",
    "        img = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Simulate lighting variation\n",
    "        lighting = np.random.uniform(0.7, 1.3)\n",
    "        img = np.clip(img * lighting, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def list_available_datasets(self):\n",
    "        \"\"\"List available datasets\"\"\"\n",
    "        print(\"\\nAvailable datasets:\")\n",
    "        print(\"=\" * 50)\n",
    "        for key, dataset in self.datasets.items():\n",
    "            print(f\"{key}: {dataset['name']}\")\n",
    "            print(f\"   Description: {dataset['description']}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "class OntologyManager:\n",
    "    \"\"\"Manages the facial recognition ontology\"\"\"\n",
    "\n",
    "    def __init__(self, ontology_file: str = \"ontology.owl\"):\n",
    "        self.ontology_file = ontology_file\n",
    "        self.graph = None\n",
    "        self.namespace = None\n",
    "\n",
    "        if ONTOLOGY_AVAILABLE:\n",
    "            self.load_ontology()\n",
    "\n",
    "    def load_ontology(self):\n",
    "        \"\"\"Load the ontology file\"\"\"\n",
    "        try:\n",
    "            self.graph = Graph()\n",
    "            if os.path.exists(self.ontology_file):\n",
    "                self.graph.parse(self.ontology_file, format=\"xml\")\n",
    "                logger.info(f\"Successfully loaded ontology from {self.ontology_file}\")\n",
    "            else:\n",
    "                # Create basic ontology structure\n",
    "                self.create_basic_ontology()\n",
    "                logger.info(\"Created basic ontology structure\")\n",
    "\n",
    "            self.namespace = Namespace(\"http://www.semanticweb.org/ontologies/facial-recognition#\")\n",
    "            self.graph.bind(\"facial\", self.namespace)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading ontology: {e}\")\n",
    "            self.graph = None\n",
    "\n",
    "    def create_basic_ontology(self):\n",
    "        \"\"\"Create a basic ontology structure\"\"\"\n",
    "        self.graph = Graph()\n",
    "        self.namespace = Namespace(\"http://www.semanticweb.org/ontologies/facial-recognition#\")\n",
    "        self.graph.bind(\"facial\", self.namespace)\n",
    "        \n",
    "        # Add basic classes\n",
    "        self.graph.add((self.namespace.Person, RDF.type, RDFS.Class))\n",
    "        self.graph.add((self.namespace.Algorithm, RDF.type, RDFS.Class))\n",
    "        self.graph.add((self.namespace.RecognitionResult, RDF.type, RDFS.Class))\n",
    "        \n",
    "        # Add properties\n",
    "        self.graph.add((self.namespace.personName, RDF.type, RDF.Property))\n",
    "        self.graph.add((self.namespace.age, RDF.type, RDF.Property))\n",
    "        self.graph.add((self.namespace.algorithmName, RDF.type, RDF.Property))\n",
    "        self.graph.add((self.namespace.accuracy, RDF.type, RDF.Property))\n",
    "        self.graph.add((self.namespace.confidenceScore, RDF.type, RDF.Property))\n",
    "\n",
    "    def add_person_to_ontology(self, person_id: str, name: str, age: int = None):\n",
    "        \"\"\"Add a person to the ontology\"\"\"\n",
    "        if not self.graph:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            person_uri = self.namespace[f\"person_{person_id}\"]\n",
    "            self.graph.add((person_uri, RDF.type, self.namespace.Person))\n",
    "            self.graph.add((person_uri, self.namespace.personName, Literal(name)))\n",
    "\n",
    "            if age:\n",
    "                self.graph.add((person_uri, self.namespace.age, Literal(age, datatype=XSD.int)))\n",
    "\n",
    "            logger.info(f\"Added person {name} to ontology\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding person to ontology: {e}\")\n",
    "\n",
    "    def add_training_result(self, model_name: str, accuracy: float):\n",
    "        \"\"\"Add training result to ontology\"\"\"\n",
    "        if not self.graph:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            result_id = f\"training_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            result_uri = self.namespace[result_id]\n",
    "\n",
    "            self.graph.add((result_uri, RDF.type, self.namespace.RecognitionResult))\n",
    "            self.graph.add((result_uri, self.namespace.confidenceScore, Literal(accuracy, datatype=XSD.double)))\n",
    "\n",
    "            # Add algorithm info\n",
    "            algorithm_uri = self.namespace[f\"algorithm_{model_name}\"]\n",
    "            self.graph.add((algorithm_uri, RDF.type, self.namespace.Algorithm))\n",
    "            self.graph.add((algorithm_uri, self.namespace.algorithmName, Literal(model_name)))\n",
    "            self.graph.add((algorithm_uri, self.namespace.accuracy, Literal(accuracy, datatype=XSD.double)))\n",
    "\n",
    "            logger.info(f\"Added training result for {model_name} with accuracy {accuracy:.4f}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding training result to ontology: {e}\")\n",
    "\n",
    "    def save_ontology(self, filename: str = None):\n",
    "        \"\"\"Save the updated ontology\"\"\"\n",
    "        if not self.graph:\n",
    "            return\n",
    "\n",
    "        filename = filename or self.ontology_file.replace('.owl', '_updated.owl')\n",
    "        try:\n",
    "            self.graph.serialize(destination=filename, format=\"xml\")\n",
    "            logger.info(f\"Ontology saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving ontology: {e}\")\n",
    "\n",
    "\n",
    "class FaceProcessor:\n",
    "    \"\"\"Face detection and preprocessing\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Load face detection cascade\n",
    "        cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "        self.face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "\n",
    "        if self.face_cascade.empty():\n",
    "            logger.error(\"Failed to load face cascade classifier\")\n",
    "\n",
    "    def detect_and_extract_face(self, image_path: str, target_size: Tuple[int, int] = (128, 128)):\n",
    "        \"\"\"Detect and extract face from image\"\"\"\n",
    "        try:\n",
    "            # Read image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                logger.warning(f\"Could not load image: {image_path}\")\n",
    "                return None\n",
    "\n",
    "            # Convert to grayscale for detection\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Detect faces\n",
    "            faces = self.face_cascade.detectMultiScale(\n",
    "                gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)\n",
    "            )\n",
    "\n",
    "            if len(faces) == 0:\n",
    "                logger.warning(f\"No face detected in: {image_path}\")\n",
    "                return None\n",
    "\n",
    "            # Use the largest face\n",
    "            if len(faces) > 1:\n",
    "                faces = sorted(faces, key=lambda x: x[2] * x[3], reverse=True)\n",
    "\n",
    "            x, y, w, h = faces[0]\n",
    "\n",
    "            # Extract face region\n",
    "            face_region = image[y:y + h, x:x + w]\n",
    "\n",
    "            # Resize to target size\n",
    "            face_resized = cv2.resize(face_region, target_size)\n",
    "\n",
    "            return face_resized\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing image {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def preprocess_image(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocess image for model input\"\"\"\n",
    "        # Normalize pixel values to [0, 1]\n",
    "        image_normalized = image.astype(np.float32) / 255.0\n",
    "        return image_normalized\n",
    "\n",
    "\n",
    "class CNNModel:\n",
    "    \"\"\"Convolutional Neural Network for face recognition\"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: Tuple[int, int, int], num_classes: int):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build CNN architecture\"\"\"\n",
    "        self.model = keras.Sequential([\n",
    "            # First Convolutional Block\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=self.input_shape),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "            # Second Convolutional Block\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "            # Third Convolutional Block\n",
    "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "            # Fourth Convolutional Block\n",
    "            layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "            # Flatten and Dense layers\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(512, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(self.num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        # Compile model\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        logger.info(\"CNN model built successfully\")\n",
    "        return self.model\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "\n",
    "        # Data augmentation\n",
    "        train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            rotation_range=15,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            zoom_range=0.1,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "\n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "        ]\n",
    "\n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            train_datagen.flow(X_train, y_train, batch_size=batch_size),\n",
    "            steps_per_epoch=len(X_train) // batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        return self.history\n",
    "\n",
    "\n",
    "class FacialRecognitionTrainer:\n",
    "    \"\"\"Main class for training facial recognition models\"\"\"\n",
    "\n",
    "    def __init__(self, ontology_file: str = \"ontology.owl\"):\n",
    "        self.ontology_manager = OntologyManager(ontology_file)\n",
    "        self.face_processor = FaceProcessor()\n",
    "        self.dataset_downloader = DatasetDownloader()\n",
    "        self.models = {}\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.class_names = []\n",
    "\n",
    "    def load_dataset(self, dataset_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load dataset from directory structure\"\"\"\n",
    "        dataset_path = Path(dataset_path)\n",
    "\n",
    "        if not dataset_path.exists():\n",
    "            logger.error(f\"Dataset path {dataset_path} does not exist\")\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "        images = []\n",
    "        labels = []\n",
    "        person_count = {}\n",
    "\n",
    "        logger.info(f\"Loading dataset from {dataset_path}\")\n",
    "\n",
    "        # Iterate through person directories\n",
    "        for person_dir in dataset_path.iterdir():\n",
    "            if not person_dir.is_dir():\n",
    "                continue\n",
    "\n",
    "            person_name = person_dir.name\n",
    "            person_count[person_name] = 0\n",
    "\n",
    "            # Add person to ontology\n",
    "            if self.ontology_manager.graph:\n",
    "                self.ontology_manager.add_person_to_ontology(\n",
    "                    person_name.replace(\" \", \"_\"), person_name\n",
    "                )\n",
    "\n",
    "            # Process images in person directory\n",
    "            for img_file in person_dir.glob(\"*\"):\n",
    "                if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "                    face_image = self.face_processor.detect_and_extract_face(str(img_file))\n",
    "\n",
    "                    if face_image is not None:\n",
    "                        processed_image = self.face_processor.preprocess_image(face_image)\n",
    "                        images.append(processed_image)\n",
    "                        labels.append(person_name)\n",
    "                        person_count[person_name] += 1\n",
    "\n",
    "        if not images:\n",
    "            logger.error(\"No valid images found in dataset\")\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        X = np.array(images)\n",
    "        y = np.array(labels)\n",
    "\n",
    "        # Encode labels\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        self.class_names = self.label_encoder.classes_.tolist()\n",
    "\n",
    "        logger.info(f\"Loaded {len(X)} images from {len(self.class_names)} persons\")\n",
    "        for person, count in person_count.items():\n",
    "            logger.info(f\"  {person}: {count} images\")\n",
    "\n",
    "        return X, y_encoded\n",
    "\n",
    "    def train_cnn_model(self, X, y, test_size=0.2, epochs=50):\n",
    "        \"\"\"Train CNN model\"\"\"\n",
    "        logger.info(\"Training CNN model...\")\n",
    "\n",
    "        # Convert labels to categorical\n",
    "        y_categorical = keras.utils.to_categorical(y, num_classes=len(self.class_names))\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y_categorical, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Training set: {len(X_train)} samples\")\n",
    "        logger.info(f\"Validation set: {len(X_val)} samples\")\n",
    "\n",
    "        # Create and train model\n",
    "        cnn_model = CNNModel(\n",
    "            input_shape=X[0].shape,\n",
    "            num_classes=len(self.class_names)\n",
    "        )\n",
    "\n",
    "        history = cnn_model.train(X_train, y_train, X_val, y_val, epochs=epochs)\n",
    "\n",
    "        # Evaluate model\n",
    "        val_loss, val_accuracy = cnn_model.model.evaluate(X_val, y_val, verbose=0)\n",
    "        logger.info(f\"CNN Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        self.models['cnn'] = cnn_model\n",
    "\n",
    "        # Add to ontology\n",
    "        if self.ontology_manager.graph:\n",
    "            self.ontology_manager.add_training_result(\"CNN\", val_accuracy)\n",
    "\n",
    "        return cnn_model, val_accuracy\n",
    "\n",
    "    def train_svm_model(self, X, y, test_size=0.2):\n",
    "        \"\"\"Train SVM model with HOG features\"\"\"\n",
    "        if not SKIMAGE_AVAILABLE:\n",
    "            logger.error(\"scikit-image not available. Cannot train SVM model.\")\n",
    "            return None, 0\n",
    "\n",
    "        logger.info(\"Training SVM model with HOG features...\")\n",
    "\n",
    "        # Extract HOG features\n",
    "        logger.info(\"Extracting HOG features...\")\n",
    "        hog_features = []\n",
    "        for img in X:\n",
    "            # Convert to grayscale if needed\n",
    "            if len(img.shape) == 3:\n",
    "                gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "            else:\n",
    "                gray = (img * 255).astype(np.uint8)\n",
    "\n",
    "            # Extract HOG features\n",
    "            features = hog(\n",
    "                gray,\n",
    "                orientations=9,\n",
    "                pixels_per_cell=(8, 8),\n",
    "                cells_per_block=(2, 2),\n",
    "                visualize=False,\n",
    "                feature_vector=True\n",
    "            )\n",
    "            hog_features.append(features)\n",
    "\n",
    "        X_hog = np.array(hog_features)\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_hog, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # Train SVM\n",
    "        svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "        svm_model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate\n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        logger.info(f\"SVM Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        self.models['svm'] = svm_model\n",
    "\n",
    "        # Add to ontology\n",
    "        if self.ontology_manager.graph:\n",
    "            self.ontology_manager.add_training_result(\"SVM\", accuracy)\n",
    "\n",
    "        return svm_model, accuracy\n",
    "\n",
    "    def export_models(self, export_dir: str = \"exported_models\"):\n",
    "        \"\"\"Export trained models and metadata\"\"\"\n",
    "        export_path = Path(export_dir)\n",
    "        export_path.mkdir(exist_ok=True)\n",
    "\n",
    "        logger.info(f\"Exporting models to {export_path}\")\n",
    "\n",
    "        # Export CNN model\n",
    "        if 'cnn' in self.models:\n",
    "            cnn_path = export_path / \"cnn_face_recognition_model.h5\"\n",
    "            self.models['cnn'].model.save(str(cnn_path))\n",
    "            logger.info(f\"CNN model exported to {cnn_path}\")\n",
    "\n",
    "        # Export SVM model\n",
    "        if 'svm' in self.models:\n",
    "            svm_path = export_path / \"svm_face_recognition_model.pkl\"\n",
    "            joblib.dump(self.models['svm'], str(svm_path))\n",
    "            logger.info(f\"SVM model exported to {svm_path}\")\n",
    "\n",
    "        # Export label encoder\n",
    "        encoder_path = export_path / \"label_encoder.pkl\"\n",
    "        joblib.dump(self.label_encoder, str(encoder_path))\n",
    "\n",
    "        # Export class names\n",
    "        classes_path = export_path / \"class_names.json\"\n",
    "        with open(classes_path, 'w') as f:\n",
    "            json.dump(self.class_names, f, indent=2)\n",
    "\n",
    "        # Export metadata\n",
    "        metadata = {\n",
    "            \"export_timestamp\": datetime.now().isoformat(),\n",
    "            \"models_exported\": list(self.models.keys()),\n",
    "            \"num_classes\": len(self.class_names),\n",
    "            \"class_names\": self.class_names,\n",
    "            \"input_shape\": [128, 128, 3] if self.models else None\n",
    "        }\n",
    "\n",
    "        metadata_path = export_path / \"model_metadata.json\"\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "        # Export updated ontology\n",
    "        if self.ontology_manager.graph:\n",
    "            ontology_path = export_path / \"ontology_updated.owl\"\n",
    "            self.ontology_manager.save_ontology(str(ontology_path))\n",
    "\n",
    "        logger.info(\"Model export completed successfully!\")\n",
    "        return str(export_path)\n",
    "\n",
    "    def predict(self, image_path: str, model_type: str = 'cnn'):\n",
    "        \"\"\"Predict person from image\"\"\"\n",
    "        if model_type not in self.models:\n",
    "            logger.error(f\"Model {model_type} not available\")\n",
    "            return None\n",
    "\n",
    "        # Process image\n",
    "        face_image = self.face_processor.detect_and_extract_face(image_path)\n",
    "        if face_image is None:\n",
    "            logger.error(\"No face detected in image\")\n",
    "            return None\n",
    "\n",
    "        processed_image = self.face_processor.preprocess_image(face_image)\n",
    "\n",
    "        if model_type == 'cnn':\n",
    "            # CNN prediction\n",
    "            image_batch = np.expand_dims(processed_image, axis=0)\n",
    "            predictions = self.models['cnn'].model.predict(image_batch, verbose=0)\n",
    "            confidence = float(np.max(predictions))\n",
    "            predicted_class = int(np.argmax(predictions))\n",
    "            person_name = self.class_names[predicted_class]\n",
    "\n",
    "        elif model_type == 'svm' and SKIMAGE_AVAILABLE:\n",
    "            # SVM prediction (requires HOG features)\n",
    "            gray = cv2.cvtColor((processed_image * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "            features = hog(\n",
    "                gray, orientations=9, pixels_per_cell=(8, 8),\n",
    "                cells_per_block=(2, 2), visualize=False, feature_vector=True\n",
    "            )\n",
    "\n",
    "            features_batch = features.reshape(1, -1)\n",
    "            probabilities = self.models['svm'].predict_proba(features_batch)[0]\n",
    "            confidence = float(np.max(probabilities))\n",
    "            predicted_class = int(np.argmax(probabilities))\n",
    "            person_name = self.class_names[predicted_class]\n",
    "\n",
    "        return {\n",
    "            'person_name': person_name,\n",
    "            'confidence': confidence,\n",
    "            'model_used': model_type\n",
    "        }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with dataset download options\"\"\"\n",
    "    print(\"Enhanced Facial Recognition Model Trainer\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = FacialRecognitionTrainer(\"ontology.owl\")\n",
    "    \n",
    "    # Dataset options\n",
    "    print(\"\\nDataset Options:\")\n",
    "    print(\"1. Use existing dataset directory\")\n",
    "    print(\"2. Create sample synthetic dataset\")\n",
    "    print(\"3. Download LFW subset (simulated)\")\n",
    "    print(\"4. List available datasets\")\n",
    "    \n",
    "    choice = input(\"\\nEnter your choice (1-4): \").strip()\n",
    "    \n",
    "    dataset_path = None\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        dataset_path = input(\"Enter dataset path (or press Enter for 'dataset'): \").strip()\n",
    "        if not dataset_path:\n",
    "            dataset_path = \"dataset\"\n",
    "    \n",
    "    elif choice == \"2\":\n",
    "        num_people = int(input(\"Number of people (default 5): \") or \"5\")\n",
    "        images_per_person = int(input(\"Images per person (default 10): \") or \"10\")\n",
    "        dataset_path = trainer.dataset_downloader.create_sample_dataset(num_people, images_per_person)\n",
    "    \n",
    "    elif choice == \"3\":\n",
    "        max_people = int(input(\"Maximum number of people (default 20): \") or \"20\")\n",
    "        dataset_path = trainer.dataset_downloader.download_lfw_subset(max_people)\n",
    "    \n",
    "    elif choice == \"4\":\n",
    "        trainer.dataset_downloader.list_available_datasets()\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        print(\"Invalid choice. Using default dataset directory.\")\n",
    "        dataset_path = \"dataset\"\n",
    "    \n",
    "    if not dataset_path:\n",
    "        print(\"Error: Could not set up dataset.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nUsing dataset: {dataset_path}\")\n",
    "    print(\"Expected structure:\")\n",
    "    print(\"dataset/\")\n",
    "    print(\"├── person1/\")\n",
    "    print(\"│   ├── img1.jpg\")\n",
    "    print(\"│   └── img2.jpg\")\n",
    "    print(\"├── person2/\")\n",
    "    print(\"│   ├── img1.jpg\")\n",
    "    print(\"│   └── img2.jpg\")\n",
    "    print(\"└── ...\")\n",
    "\n",
    "    # Load dataset\n",
    "    X, y = trainer.load_dataset(dataset_path)\n",
    "\n",
    "    if len(X) == 0:\n",
    "        print(\"No data loaded. Please check your dataset structure.\")\n",
    "        return\n",
    "\n",
    "    # Training options\n",
    "    print(f\"\\nTraining models with {len(X)} images from {len(trainer.class_names)} persons...\")\n",
    "    \n",
    "    train_cnn = input(\"Train CNN model? (y/n, default y): \").strip().lower()\n",
    "    train_cnn = train_cnn != 'n'\n",
    "    \n",
    "    train_svm = input(\"Train SVM model? (y/n, default y): \").strip().lower()\n",
    "    train_svm = train_svm != 'n' and SKIMAGE_AVAILABLE\n",
    "    \n",
    "    if not train_svm and not SKIMAGE_AVAILABLE:\n",
    "        print(\"Note: SVM training disabled - scikit-image not available\")\n",
    "    \n",
    "    epochs = int(input(\"Number of epochs for CNN (default 30): \") or \"30\")\n",
    "\n",
    "    # Train models\n",
    "    cnn_accuracy = 0\n",
    "    svm_accuracy = 0\n",
    "    \n",
    "    if train_cnn:\n",
    "        cnn_model, cnn_accuracy = trainer.train_cnn_model(X, y, epochs=epochs)\n",
    "\n",
    "    if train_svm:\n",
    "        svm_model, svm_accuracy = trainer.train_svm_model(X, y)\n",
    "\n",
    "    # Export models\n",
    "    export_path = trainer.export_models()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING COMPLETED!\")\n",
    "    print(\"=\" * 60)\n",
    "    if train_cnn:\n",
    "        print(f\"CNN Accuracy: {cnn_accuracy:.4f}\")\n",
    "    if train_svm:\n",
    "        print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
    "    print(f\"Models exported to: {export_path}\")\n",
    "    print(\"\\nExported files:\")\n",
    "    if train_cnn:\n",
    "        print(\"├── cnn_face_recognition_model.h5\")\n",
    "    if train_svm:\n",
    "        print(\"├── svm_face_recognition_model.pkl\")\n",
    "    print(\"├── label_encoder.pkl\")\n",
    "    print(\"├── class_names.json\")\n",
    "    print(\"├── model_metadata.json\")\n",
    "    print(\"└── ontology_updated.owl\")\n",
    "\n",
    "    # Test prediction\n",
    "    test_image = input(\"\\nEnter test image path (or press Enter to skip): \").strip()\n",
    "    if test_image and os.path.exists(test_image):\n",
    "        print(f\"\\nTesting prediction on: {test_image}\")\n",
    "\n",
    "        # Test CNN\n",
    "        if train_cnn:\n",
    "            cnn_result = trainer.predict(test_image, 'cnn')\n",
    "            if cnn_result:\n",
    "                print(f\"CNN Prediction: {cnn_result['person_name']} (confidence: {cnn_result['confidence']:.3f})\")\n",
    "\n",
    "        # Test SVM\n",
    "        if train_svm:\n",
    "            svm_result = trainer.predict(test_image, 'svm')\n",
    "            if svm_result:\n",
    "                print(f\"SVM Prediction: {svm_result['person_name']} (confidence: {svm_result['confidence']:.3f})\")\n",
    "\n",
    "    print(\"\\nTraining completed successfully!\")\n",
    "    print(\"To use the trained models, load them using:\")\n",
    "    print(\"- CNN: tf.keras.models.load_model('exported_models/cnn_face_recognition_model.h5')\")\n",
    "    print(\"- SVM: joblib.load('exported_models/svm_face_recognition_model.pkl')\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Install required packages\n",
    "    required_packages = [\n",
    "        \"tensorflow>=2.8.0\",\n",
    "        \"opencv-python>=4.5.0\",\n",
    "        \"scikit-learn>=1.0.0\",\n",
    "        \"scikit-image>=0.19.0\",\n",
    "        \"rdflib>=6.0.0\",\n",
    "        \"joblib>=1.1.0\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Required packages:\")\n",
    "    for package in required_packages:\n",
    "        print(f\"  pip install {package}\")\n",
    "    print(\"\\nRun: pip install \" + \" \".join(required_packages))\n",
    "    print()\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c6602f-7c32-4ec0-aae7-7da8d353c2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df1ac3-17e1-4d76-b128-f357c3a85478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
