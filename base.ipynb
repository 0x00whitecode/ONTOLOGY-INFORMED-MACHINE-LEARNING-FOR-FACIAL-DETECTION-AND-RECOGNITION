{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "566a26e5-f65f-441e-81fc-9b984de706b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpatches\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rectangle\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mmtcnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmtcnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MTCNN\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Directory paths\u001b[39;00m\n\u001b[32m      5\u001b[39m train_directory = \u001b[33m'\u001b[39m\u001b[33m/kaggle/working/train\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/mtcnn/__init__.py:23\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# MIT License\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Copyright (c) 2019-2024 Iván de Paz Centeno\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# SOFTWARE.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mmtcnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmtcnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MTCNN\n\u001b[32m     25\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mMTCNN\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/mtcnn/mtcnn.py:23\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# MIT License\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Copyright (c) 2019-2024 Iván de Paz Centeno\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# SOFTWARE.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mtf\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mmtcnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StagePNet, StageRNet, StageONet\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "# Directory paths\n",
    "train_directory = '/kaggle/working/train'\n",
    "\n",
    "# Get a list of all folders in the train directory\n",
    "all_folders = [folder for folder in os.listdir(train_directory) if os.path.isdir(os.path.join(train_directory, folder))]\n",
    "\n",
    "# Randomly select two folders\n",
    "selected_folders = random.sample(all_folders, 2)\n",
    "\n",
    "# Initialize MTCNN detector\n",
    "detector = MTCNN()\n",
    "\n",
    "# Iterate through selected folders\n",
    "for folder in selected_folders:\n",
    "    folder_path = os.path.join(train_directory, folder)\n",
    "    \n",
    "    # Get a list of all images in the folder\n",
    "    all_images = [img for img in os.listdir(folder_path) if img.endswith('.jpg')]\n",
    "    \n",
    "    # Randomly select one image\n",
    "    selected_image = random.choice(all_images)\n",
    "    image_path = os.path.join(folder_path, selected_image)\n",
    "    \n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Detect faces using MTCNN\n",
    "    faces = detector.detect_faces(image)\n",
    "    \n",
    "    # Display image with bounding boxes around detected faces\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    \n",
    "    # Display original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.title('Original Image')\n",
    "    \n",
    "    # Display image with bounding boxes\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image_rgb)\n",
    "    \n",
    "    for face in faces:\n",
    "        x, y, width, height = face['box']\n",
    "        rect = Rectangle((x, y), width, height, fill=False, color='red')\n",
    "        plt.gca().add_patch(rect)\n",
    "        \n",
    "        # Display additional keypoints\n",
    "        for key, value in face['keypoints'].items():\n",
    "            plt.scatter(value[0], value[1], s=30, color='blue', marker='o')\n",
    "            plt.text(value[0] + 5, value[1], key, color='blue')\n",
    "    \n",
    "    plt.title('Detected Faces with Keypoints')\n",
    "    plt.show()\n",
    "\n",
    "    # Display metadata of detected faces\n",
    "    print(f\"Metadata of detected faces in {folder}/{selected_image}:\")\n",
    "    for i, face in enumerate(faces):\n",
    "        print(f\"Face {i + 1}:\")\n",
    "        print(f\"   Confidence: {face['confidence']:.2f}\")\n",
    "        print(f\"   Bounding Box: {face['box']}\")\n",
    "        print(f\"   Keypoints: {face['keypoints']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc96f333-a738-49d0-97f1-9a851f7e90d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mflask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flask, render_template, request, jsonify, send_from_directory\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mmtcnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MTCNN\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, jsonify, send_from_directory\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['UPLOAD_FOLDER'] = 'uploads'\n",
    "app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size\n",
    "\n",
    "# Create upload directory if it doesn't exist\n",
    "os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n",
    "\n",
    "# Initialize MTCNN detector\n",
    "detector = MTCNN(device=\"CPU:0\")\n",
    "\n",
    "# Face Detection Ontology\n",
    "FACE_ONTOLOGY = {\n",
    "    \"Face\": {\n",
    "        \"description\": \"A human facial region detected in an image\",\n",
    "        \"properties\": {\n",
    "            \"bounding_box\": {\n",
    "                \"type\": \"Rectangle\",\n",
    "                \"description\": \"Rectangular boundary containing the face\",\n",
    "                \"coordinates\": [\"x\", \"y\", \"width\", \"height\"]\n",
    "            },\n",
    "            \"confidence\": {\n",
    "                \"type\": \"Float\",\n",
    "                \"description\": \"Detection confidence score (0.0 to 1.0)\",\n",
    "                \"range\": [0.0, 1.0]\n",
    "            },\n",
    "            \"keypoints\": {\n",
    "                \"type\": \"FacialKeypoints\",\n",
    "                \"description\": \"Anatomical landmarks on the face\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"FacialKeypoints\": {\n",
    "        \"description\": \"Key anatomical points on a human face\",\n",
    "        \"landmarks\": {\n",
    "            \"left_eye\": {\n",
    "                \"description\": \"Center of the left eye (from viewer's perspective)\",\n",
    "                \"anatomical_region\": \"Ocular\",\n",
    "                \"coordinates\": [\"x\", \"y\"]\n",
    "            },\n",
    "            \"right_eye\": {\n",
    "                \"description\": \"Center of the right eye (from viewer's perspective)\",\n",
    "                \"anatomical_region\": \"Ocular\",\n",
    "                \"coordinates\": [\"x\", \"y\"]\n",
    "            },\n",
    "            \"nose\": {\n",
    "                \"description\": \"Tip of the nose\",\n",
    "                \"anatomical_region\": \"Nasal\",\n",
    "                \"coordinates\": [\"x\", \"y\"]\n",
    "            },\n",
    "            \"mouth_left\": {\n",
    "                \"description\": \"Left corner of the mouth\",\n",
    "                \"anatomical_region\": \"Oral\",\n",
    "                \"coordinates\": [\"x\", \"y\"]\n",
    "            },\n",
    "            \"mouth_right\": {\n",
    "                \"description\": \"Right corner of the mouth\",\n",
    "                \"anatomical_region\": \"Oral\",\n",
    "                \"coordinates\": [\"x\", \"y\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def allowed_file(filename):\n",
    "    \"\"\"Check if file extension is allowed\"\"\"\n",
    "    ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif', 'bmp'}\n",
    "    return '.' in filename and \\\n",
    "        filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "\n",
    "def draw_detections(image, detections):\n",
    "    \"\"\"Draw bounding boxes and keypoints on the image\"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Define colors\n",
    "    box_color = (0, 255, 0)  # Green for bounding box\n",
    "    keypoint_colors = {\n",
    "        'left_eye': (255, 0, 0),  # Red\n",
    "        'right_eye': (255, 0, 0),  # Red\n",
    "        'nose': (0, 0, 255),  # Blue\n",
    "        'mouth_left': (255, 255, 0),  # Yellow\n",
    "        'mouth_right': (255, 255, 0)  # Yellow\n",
    "    }\n",
    "\n",
    "    for detection in detections:\n",
    "        # Draw bounding box\n",
    "        box = detection['box']\n",
    "        x, y, w, h = box\n",
    "        draw.rectangle([x, y, x + w, y + h], outline=box_color, width=3)\n",
    "\n",
    "        # Draw confidence score\n",
    "        confidence_text = f\"Confidence: {detection['confidence']:.3f}\"\n",
    "        draw.text((x, y - 20), confidence_text, fill=box_color)\n",
    "\n",
    "        # Draw keypoints\n",
    "        keypoints = detection['keypoints']\n",
    "        for keypoint_name, (kx, ky) in keypoints.items():\n",
    "            color = keypoint_colors.get(keypoint_name, (255, 255, 255))\n",
    "            # Draw circle for keypoint\n",
    "            radius = 4\n",
    "            draw.ellipse([kx - radius, ky - radius, kx + radius, ky + radius],\n",
    "                         fill=color, outline=(0, 0, 0), width=1)\n",
    "            # Draw label\n",
    "            draw.text((kx + 5, ky - 10), keypoint_name, fill=color)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def annotate_with_ontology(detections):\n",
    "    \"\"\"Add ontological information to detections\"\"\"\n",
    "    annotated_results = []\n",
    "\n",
    "    for i, detection in enumerate(detections):\n",
    "        annotated_detection = {\n",
    "            \"face_id\": i + 1,\n",
    "            \"ontology_class\": \"Face\",\n",
    "            \"detection_data\": detection,\n",
    "            \"semantic_annotation\": {\n",
    "                \"anatomical_regions\": {\n",
    "                    \"ocular\": {\n",
    "                        \"left_eye\": detection['keypoints']['left_eye'],\n",
    "                        \"right_eye\": detection['keypoints']['right_eye']\n",
    "                    },\n",
    "                    \"nasal\": {\n",
    "                        \"nose\": detection['keypoints']['nose']\n",
    "                    },\n",
    "                    \"oral\": {\n",
    "                        \"mouth_left\": detection['keypoints']['mouth_left'],\n",
    "                        \"mouth_right\": detection['keypoints']['mouth_right']\n",
    "                    }\n",
    "                },\n",
    "                \"quality_metrics\": {\n",
    "                    \"detection_confidence\": detection['confidence'],\n",
    "                    \"quality_assessment\": \"high\" if detection['confidence'] > 0.95 else \"medium\" if detection[\n",
    "                                                                                                        'confidence'] > 0.8 else \"low\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        annotated_results.append(annotated_detection)\n",
    "\n",
    "    return annotated_results\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "@app.route('/ontology')\n",
    "def get_ontology():\n",
    "    \"\"\"Return the face detection ontology\"\"\"\n",
    "    return jsonify(FACE_ONTOLOGY)\n",
    "\n",
    "\n",
    "@app.route('/upload', methods=['POST'])\n",
    "def upload_file():\n",
    "    if 'file' not in request.files:\n",
    "        return jsonify({'error': 'No file selected'}), 400\n",
    "\n",
    "    file = request.files['file']\n",
    "    if file.filename == '':\n",
    "        return jsonify({'error': 'No file selected'}), 400\n",
    "\n",
    "    if file and allowed_file(file.filename):\n",
    "        try:\n",
    "            # Read image\n",
    "            image_bytes = file.read()\n",
    "            image = Image.open(BytesIO(image_bytes))\n",
    "\n",
    "            # Convert to RGB if necessary\n",
    "            if image.mode != 'RGB':\n",
    "                image = image.convert('RGB')\n",
    "\n",
    "            # Convert PIL image to numpy array for MTCNN\n",
    "            image_array = np.array(image)\n",
    "\n",
    "            # Detect faces\n",
    "            detections = detector.detect_faces(image_array)\n",
    "\n",
    "            if not detections:\n",
    "                return jsonify({\n",
    "                    'message': 'No faces detected in the image',\n",
    "                    'detections': [],\n",
    "                    'annotated_results': []\n",
    "                })\n",
    "\n",
    "            # Draw detections on image\n",
    "            annotated_image = draw_detections(image.copy(), detections)\n",
    "\n",
    "            # Convert annotated image to base64\n",
    "            buffered = BytesIO()\n",
    "            annotated_image.save(buffered, format=\"JPEG\")\n",
    "            img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "            # Add ontological annotations\n",
    "            annotated_results = annotate_with_ontology(detections)\n",
    "\n",
    "            return jsonify({\n",
    "                'success': True,\n",
    "                'message': f'Detected {len(detections)} face(s)',\n",
    "                'detections': detections,\n",
    "                'annotated_results': annotated_results,\n",
    "                'annotated_image': f'data:image/jpeg;base64,{img_str}'\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': f'Error processing image: {str(e)}'}), 500\n",
    "\n",
    "    return jsonify({'error': 'Invalid file type'}), 400\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create templates directory and index.html if they don't exist\n",
    "    os.makedirs('templates', exist_ok=True)\n",
    "\n",
    "    html_content = '''\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>MTCNN Face Detection with Ontology</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            max-width: 1200px;\n",
    "            margin: 0 auto;\n",
    "            padding: 20px;\n",
    "            background-color: #f5f5f5;\n",
    "        }\n",
    "        .container {\n",
    "            background: white;\n",
    "            padding: 30px;\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
    "        }\n",
    "        h1 {\n",
    "            color: #333;\n",
    "            text-align: center;\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        .upload-section {\n",
    "            border: 2px dashed #ddd;\n",
    "            border-radius: 10px;\n",
    "            padding: 40px;\n",
    "            text-align: center;\n",
    "            margin-bottom: 30px;\n",
    "            transition: border-color 0.3s;\n",
    "        }\n",
    "        .upload-section:hover {\n",
    "            border-color: #007bff;\n",
    "        }\n",
    "        .file-input {\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "        .btn {\n",
    "            background-color: #007bff;\n",
    "            color: white;\n",
    "            padding: 12px 24px;\n",
    "            border: none;\n",
    "            border-radius: 5px;\n",
    "            cursor: pointer;\n",
    "            font-size: 16px;\n",
    "            transition: background-color 0.3s;\n",
    "        }\n",
    "        .btn:hover {\n",
    "            background-color: #0056b3;\n",
    "        }\n",
    "        .btn:disabled {\n",
    "            background-color: #6c757d;\n",
    "            cursor: not-allowed;\n",
    "        }\n",
    "        .results {\n",
    "            margin-top: 30px;\n",
    "        }\n",
    "        .result-image {\n",
    "            max-width: 100%;\n",
    "            height: auto;\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 2px 10px rgba(0,0,0,0.1);\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "        .detection-info {\n",
    "            background: #f8f9fa;\n",
    "            padding: 20px;\n",
    "            border-radius: 10px;\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "        .face-card {\n",
    "            background: white;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 8px;\n",
    "            padding: 15px;\n",
    "            margin: 10px 0;\n",
    "        }\n",
    "        .keypoint {\n",
    "            display: inline-block;\n",
    "            background: #e9ecef;\n",
    "            padding: 5px 10px;\n",
    "            border-radius: 15px;\n",
    "            margin: 3px;\n",
    "            font-size: 12px;\n",
    "        }\n",
    "        .confidence {\n",
    "            font-weight: bold;\n",
    "            color: #28a745;\n",
    "        }\n",
    "        .ontology-section {\n",
    "            background: #fff3cd;\n",
    "            border: 1px solid #ffeaa7;\n",
    "            border-radius: 10px;\n",
    "            padding: 20px;\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "        .loading {\n",
    "            display: none;\n",
    "            text-align: center;\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "        .spinner {\n",
    "            border: 4px solid #f3f3f3;\n",
    "            border-top: 4px solid #007bff;\n",
    "            border-radius: 50%;\n",
    "            width: 40px;\n",
    "            height: 40px;\n",
    "            animation: spin 1s linear infinite;\n",
    "            margin: 0 auto;\n",
    "        }\n",
    "        @keyframes spin {\n",
    "            0% { transform: rotate(0deg); }\n",
    "            100% { transform: rotate(360deg); }\n",
    "        }\n",
    "        .error {\n",
    "            color: #dc3545;\n",
    "            background: #f8d7da;\n",
    "            border: 1px solid #f5c6cb;\n",
    "            padding: 15px;\n",
    "            border-radius: 5px;\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "        .success {\n",
    "            color: #155724;\n",
    "            background: #d4edda;\n",
    "            border: 1px solid #c3e6cb;\n",
    "            padding: 15px;\n",
    "            border-radius: 5px;\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>🔍 MTCNN Face Detection with Ontology</h1>\n",
    "\n",
    "        <div class=\"upload-section\">\n",
    "            <h3>Upload an Image for Face Detection</h3>\n",
    "            <p>Supported formats: JPG, JPEG, PNG, GIF, BMP</p>\n",
    "            <input type=\"file\" id=\"imageFile\" class=\"file-input\" accept=\"image/*\">\n",
    "            <br>\n",
    "            <button onclick=\"uploadImage()\" class=\"btn\" id=\"uploadBtn\">\n",
    "                Detect Faces\n",
    "            </button>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"loading\" id=\"loading\">\n",
    "            <div class=\"spinner\"></div>\n",
    "            <p>Processing image...</p>\n",
    "        </div>\n",
    "\n",
    "        <div id=\"results\" class=\"results\"></div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        function uploadImage() {\n",
    "            const fileInput = document.getElementById('imageFile');\n",
    "            const file = fileInput.files[0];\n",
    "\n",
    "            if (!file) {\n",
    "                alert('Please select an image file first.');\n",
    "                return;\n",
    "            }\n",
    "\n",
    "            const formData = new FormData();\n",
    "            formData.append('file', file);\n",
    "\n",
    "            // Show loading\n",
    "            document.getElementById('loading').style.display = 'block';\n",
    "            document.getElementById('uploadBtn').disabled = true;\n",
    "            document.getElementById('results').innerHTML = '';\n",
    "\n",
    "            fetch('/upload', {\n",
    "                method: 'POST',\n",
    "                body: formData\n",
    "            })\n",
    "            .then(response => response.json())\n",
    "            .then(data => {\n",
    "                // Hide loading\n",
    "                document.getElementById('loading').style.display = 'none';\n",
    "                document.getElementById('uploadBtn').disabled = false;\n",
    "\n",
    "                if (data.error) {\n",
    "                    document.getElementById('results').innerHTML = \n",
    "                        `<div class=\"error\">Error: ${data.error}</div>`;\n",
    "                    return;\n",
    "                }\n",
    "\n",
    "                displayResults(data);\n",
    "            })\n",
    "            .catch(error => {\n",
    "                console.error('Error:', error);\n",
    "                document.getElementById('loading').style.display = 'none';\n",
    "                document.getElementById('uploadBtn').disabled = false;\n",
    "                document.getElementById('results').innerHTML = \n",
    "                    `<div class=\"error\">Error uploading image: ${error.message}</div>`;\n",
    "            });\n",
    "        }\n",
    "\n",
    "        function displayResults(data) {\n",
    "            const resultsDiv = document.getElementById('results');\n",
    "\n",
    "            if (data.detections.length === 0) {\n",
    "                resultsDiv.innerHTML = `\n",
    "                    <div class=\"success\">${data.message}</div>\n",
    "                `;\n",
    "                return;\n",
    "            }\n",
    "\n",
    "            let html = `\n",
    "                <div class=\"success\">${data.message}</div>\n",
    "\n",
    "                <h3>📸 Annotated Image</h3>\n",
    "                <img src=\"${data.annotated_image}\" alt=\"Annotated Image\" class=\"result-image\">\n",
    "\n",
    "                <h3>🎯 Detection Results</h3>\n",
    "            `;\n",
    "\n",
    "            data.annotated_results.forEach((result, index) => {\n",
    "                const detection = result.detection_data;\n",
    "                const semantic = result.semantic_annotation;\n",
    "\n",
    "                html += `\n",
    "                    <div class=\"face-card\">\n",
    "                        <h4>Face ${result.face_id}</h4>\n",
    "                        <p><strong>Confidence:</strong> <span class=\"confidence\">${(detection.confidence * 100).toFixed(1)}%</span></p>\n",
    "                        <p><strong>Quality:</strong> ${semantic.quality_metrics.quality_assessment}</p>\n",
    "                        <p><strong>Bounding Box:</strong> [${detection.box.join(', ')}]</p>\n",
    "\n",
    "                        <h5>🔍 Facial Keypoints:</h5>\n",
    "                        <div>\n",
    "                `;\n",
    "\n",
    "                Object.entries(detection.keypoints).forEach(([name, coords]) => {\n",
    "                    html += `<span class=\"keypoint\">${name.replace('_', ' ')}: (${coords[0]}, ${coords[1]})</span>`;\n",
    "                });\n",
    "\n",
    "                html += `\n",
    "                        </div>\n",
    "\n",
    "                        <h5>🧠 Ontological Classification:</h5>\n",
    "                        <p><strong>Class:</strong> ${result.ontology_class}</p>\n",
    "                        <p><strong>Anatomical Regions:</strong></p>\n",
    "                        <ul>\n",
    "                            <li><strong>Ocular:</strong> Left eye (${semantic.anatomical_regions.ocular.left_eye}), Right eye (${semantic.anatomical_regions.ocular.right_eye})</li>\n",
    "                            <li><strong>Nasal:</strong> Nose (${semantic.anatomical_regions.nasal.nose})</li>\n",
    "                            <li><strong>Oral:</strong> Mouth left (${semantic.anatomical_regions.oral.mouth_left}), Mouth right (${semantic.anatomical_regions.oral.mouth_right})</li>\n",
    "                        </ul>\n",
    "                    </div>\n",
    "                `;\n",
    "            });\n",
    "\n",
    "            html += `\n",
    "                <div class=\"ontology-section\">\n",
    "                    <h3>📚 Ontology Information</h3>\n",
    "                    <p>This application uses a structured ontology to classify and annotate facial features:</p>\n",
    "                    <ul>\n",
    "                        <li><strong>Face:</strong> The main detected object with bounding box, confidence, and keypoints</li>\n",
    "                        <li><strong>Anatomical Regions:</strong> Ocular (eyes), Nasal (nose), and Oral (mouth) regions</li>\n",
    "                        <li><strong>Quality Metrics:</strong> Assessment based on detection confidence</li>\n",
    "                    </ul>\n",
    "                    <p><small>View the complete ontology structure at: <a href=\"/ontology\" target=\"_blank\">/ontology</a></small></p>\n",
    "                </div>\n",
    "            `;\n",
    "\n",
    "            resultsDiv.innerHTML = html;\n",
    "        }\n",
    "\n",
    "        // Allow drag and drop\n",
    "        const uploadSection = document.querySelector('.upload-section');\n",
    "\n",
    "        uploadSection.addEventListener('dragover', (e) => {\n",
    "            e.preventDefault();\n",
    "            uploadSection.style.borderColor = '#007bff';\n",
    "        });\n",
    "\n",
    "        uploadSection.addEventListener('dragleave', (e) => {\n",
    "            e.preventDefault();\n",
    "            uploadSection.style.borderColor = '#ddd';\n",
    "        });\n",
    "\n",
    "        uploadSection.addEventListener('drop', (e) => {\n",
    "            e.preventDefault();\n",
    "            uploadSection.style.borderColor = '#ddd';\n",
    "\n",
    "            const files = e.dataTransfer.files;\n",
    "            if (files.length > 0) {\n",
    "                document.getElementById('imageFile').files = files;\n",
    "            }\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "    '''\n",
    "\n",
    "    with open('templates/index.html', 'w') as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "    print(\"Starting Flask app...\")\n",
    "    print(\"Make sure to install required packages:\")\n",
    "    print(\"pip install flask mtcnn pillow opencv-python\")\n",
    "    print(\"\\nAccess the application at: http://localhost:5000\")\n",
    "\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7baa0-bf5b-4129-b112-466568e6cbf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
